{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acefb740-90e4-4e80-8fee-337d1120c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : This file implements the Trainer class\n",
    "Author      : https://github.com/donglee-afar\n",
    "License     : MIT\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from log import log_dataset\n",
    "from sample import sliding_window\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import save_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706ae98-7f10-40f5-ba32-a92f61824cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, options):\n",
    "        self.model_name = options[\"model_name\"]\n",
    "        self.save_dir = options[\"save_dir\"]\n",
    "        self.data_dir = options[\"data_dir\"]\n",
    "        self.window_size = options[\"window_size\"]\n",
    "        self.batch_size = options[\"batch_size\"] \n",
    "        self.device = options[\"device\"]\n",
    "        self.lr_step = options[\"lr_step\"]\n",
    "        self.lr_decay_ratio = options[\"lr_decay_ratio\"]\n",
    "        self.accumulation_step = options[\"accumulation_step\"]\n",
    "        self.max_epoch = options[\"max_epoch\"]\n",
    "        self.sequentials = options[\"sequentials\"]\n",
    "        self.sample = options[\"sample\"]\n",
    "        self.num_classes = options[\"num_classes\"]\n",
    "        self.padding_option = options[\"padding\"]\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        if self.sample == \"sliding_window\":\n",
    "            train_logs, train_labels = sliding_window(\n",
    "                self.data_dir,\n",
    "                datatype=\"train\",\n",
    "                window_size=self.window_size,\n",
    "                num_classes=self.num_classes,\n",
    "                padding_option=self.padding_option,\n",
    "            )\n",
    "            val_logs, val_labels = sliding_window(\n",
    "                self.data_dir,\n",
    "                datatype=\"val\",\n",
    "                window_size=self.window_size,\n",
    "                num_classes=self.num_classes,\n",
    "                padding_option=self.padding_option,\n",
    "                sample_ratio=0.001,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        train_dataset = log_dataset(\n",
    "            logs=train_logs, labels=train_labels, seq=self.sequentials\n",
    "        )\n",
    "        valid_dataset = log_dataset(\n",
    "            logs=val_logs, labels=val_labels, seq=self.sequentials\n",
    "        )\n",
    "\n",
    "        del train_logs\n",
    "        del val_logs\n",
    "        gc.collect()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True, pin_memory=True\n",
    "        )\n",
    "        self.valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        if options[\"optimizer\"] == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(\n",
    "                self.model.parameters(), lr=options[\"lr\"], momentum=0.9\n",
    "            )\n",
    "        elif options[\"optimizer\"] == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=options[\"lr\"],\n",
    "                betas=(0.9, 0.999),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.best_loss = 1e10\n",
    "        self.best_score = -1\n",
    "        save_parameters(options, self.save_dir + \"parameters.txt\")\n",
    "        self.log = {\n",
    "            \"train\": {key: [] for key in [\"epoch\", \"lr\", \"time\", \"loss\"]},\n",
    "            \"valid\": {key: [] for key in [\"epoch\", \"lr\", \"time\", \"loss\"]},\n",
    "        }\n",
    "        if options[\"resume_path\"] is not None:\n",
    "            if os.path.isfile(options[\"resume_path\"]):\n",
    "                self.resume(options[\"resume_path\"], load_optimizer=True)\n",
    "            else:\n",
    "                print(\"Checkpoint not found\")\n",
    "\n",
    "    def resume(self, path, load_optimizer=True):\n",
    "        print(\"Resuming from {}\".format(path))\n",
    "        checkpoint = torch.load(path)\n",
    "        self.start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        self.best_loss = checkpoint[\"best_loss\"]\n",
    "        self.log = checkpoint[\"log\"]\n",
    "        self.best_f1_score = checkpoint[\"best_f1_score\"]\n",
    "        self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        if \"optimizer\" in checkpoint.keys() and load_optimizer:\n",
    "            print(\"Loading optimizer state dict\")\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    def save_checkpoint(self, epoch, save_optimizer=True, suffix=\"\"):\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"best_loss\": self.best_loss,\n",
    "            \"log\": self.log,\n",
    "            \"best_score\": self.best_score,\n",
    "        }\n",
    "        if save_optimizer:\n",
    "            checkpoint[\"optimizer\"] = self.optimizer.state_dict()\n",
    "        save_path = self.save_dir + self.model_name + \"_\" + suffix + \".pth\"\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(\"Save model checkpoint at {}\".format(save_path))\n",
    "\n",
    "    def save_log(self):\n",
    "        try:\n",
    "            for key, values in self.log.items():\n",
    "                pd.DataFrame(values).to_csv(\n",
    "                    self.save_dir + key + \"_log.csv\", index=False\n",
    "                )\n",
    "            print(\"Log saved\")\n",
    "        except:\n",
    "            print(\"Failed to save logs\")\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.log[\"train\"][\"epoch\"].append(epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        lr = self.optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        print(\n",
    "            \"Starting epoch: %d | phase: train | ⏰: %s | Learning rate: %f\"\n",
    "            % (epoch, start, lr)\n",
    "        )\n",
    "        self.log[\"train\"][\"lr\"].append(lr)\n",
    "        self.log[\"train\"][\"time\"].append(start)\n",
    "        self.model.train()\n",
    "        # Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "        self.optimizer.zero_grad()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        tbar = tqdm(self.train_loader, desc=\"\\r\")\n",
    "        num_batch = len(self.train_loader)\n",
    "        total_losses = 0\n",
    "        for i, (log, label) in enumerate(tbar):\n",
    "            features = []\n",
    "            for value in log.values():\n",
    "                features.append(value.clone().detach().to(self.device))\n",
    "            output = self.model(features=features, device=self.device)\n",
    "            loss = criterion(output, label.to(self.device))\n",
    "            total_losses += float(loss)\n",
    "            loss /= self.accumulation_step\n",
    "            # Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "            loss.backward()\n",
    "            if (i + 1) % self.accumulation_step == 0:\n",
    "                # Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.\n",
    "                self.optimizer.step()\n",
    "                # before next iteration we must call zero_grad() -> empties the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "            tbar.set_description(\"Train loss: %.5f\" % (total_losses / (i + 1)))\n",
    "\n",
    "        self.log[\"train\"][\"loss\"].append(total_losses / num_batch)\n",
    "\n",
    "    def valid(self, epoch):\n",
    "        self.model.eval()\n",
    "        self.log[\"valid\"][\"epoch\"].append(epoch)\n",
    "        lr = self.optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        self.log[\"valid\"][\"lr\"].append(lr)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(\"Starting epoch: %d | phase: valid | ⏰: %s \" % (epoch, start))\n",
    "        self.log[\"valid\"][\"time\"].append(start)\n",
    "        total_losses = 0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        tbar = tqdm(self.valid_loader, desc=\"\\r\")\n",
    "        num_batch = len(self.valid_loader)\n",
    "        for i, (log, label) in enumerate(tbar):\n",
    "            with torch.no_grad():\n",
    "                features = []\n",
    "                for value in log.values():\n",
    "                    features.append(value.clone().detach().to(self.device))\n",
    "                output = self.model(features=features, device=self.device)\n",
    "                loss = criterion(output, label.to(self.device))\n",
    "                total_losses += float(loss)\n",
    "        print(\"Validation loss:\", total_losses / num_batch)\n",
    "        self.log[\"valid\"][\"loss\"].append(total_losses / num_batch)\n",
    "\n",
    "        if total_losses / num_batch < self.best_loss:\n",
    "            self.best_loss = total_losses / num_batch\n",
    "            self.save_checkpoint(epoch, save_optimizer=False, suffix=\"bestloss\")\n",
    "\n",
    "    def start_train(self):\n",
    "        for epoch in range(self.start_epoch, self.max_epoch):\n",
    "            if epoch == 0:\n",
    "                self.optimizer.param_groups[0][\"lr\"] /= 32\n",
    "            if epoch in [1, 2, 3, 4, 5]:\n",
    "                self.optimizer.param_groups[0][\"lr\"] *= 2\n",
    "            if epoch in self.lr_step:\n",
    "                self.optimizer.param_groups[0][\"lr\"] *= self.lr_decay_ratio\n",
    "            self.train(epoch)\n",
    "            # self.valid(epoch)\n",
    "            # hdfs             if epoch >= self.max_epoch // 2 and epoch % 10 == 0:\n",
    "            if epoch >= self.max_epoch // 2 and epoch % 2 == 0:\n",
    "                self.valid(epoch)\n",
    "                self.save_checkpoint(\n",
    "                    epoch, save_optimizer=True, suffix=\"epoch\" + str(epoch)\n",
    "                )\n",
    "            self.save_checkpoint(epoch, save_optimizer=True, suffix=\"last\")\n",
    "            self.save_log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
